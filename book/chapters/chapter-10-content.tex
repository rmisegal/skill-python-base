% chapter-10-content.tex
% Main content for Chapter 10 - included by standalone file
% Note: This file contains sections only (no \chapter or learning objectives)

\section{×¤×ª×™×—×”: ×¢×•×œ× ×©×œ ×‘×—×™×¨×•×ª}

×œ×¤× ×™ ×××ª×™×™× ×©× ×”, ×‘×¢×™×“×Ÿ ×”××”×¤×›×” ×”×ª×¢×©×™×™×ª×™×ª, ×¢××“×• ×™×–××™× ×‘×¤× ×™ ×©××œ×” ×¤×©×•×˜×” ××š ×”×¨×ª ×’×•×¨×œ: ×”×× ×œ×¨×›×•×© ×× ×•×¢ ×§×™×˜×•×¨ ××¡×•×’ ×' ××• ××¡×•×’ ×‘'? ×”×”×—×œ×˜×” ×”×–×•, ×©× ×¨××ª×” ×˜×›× ×™×ª ×‘×œ×‘×“, ×§×‘×¢×” ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ××ª ×¢×ª×™×“×• ×©×œ ×‘×™×ª ×”××œ××›×”. ×× ×•×¢ ×™×¢×™×œ ××“×™ ×™×›×•×œ ×”×™×” ×œ×™×¦×•×¨ ×¢×œ×•×™×•×ª ×ª×—×–×•×§×” ××•×¤×¨×–×•×ª, ×‘×¢×•×“ ×× ×•×¢ ×—×œ×© ××“×™ ×œ× ×™×›×•×œ ×”×™×” ×œ×¢× ×•×ª ×¢×œ ×“×¨×™×©×•×ª ×”×™×™×¦×•×¨ ×”×’×“×œ×•×ª. ×•×”×›×™ ×’×¨×•×¢ ××›×œ -- ×‘×—×™×¨×” ×‘×× ×•×¢ "×œ× × ×›×•×Ÿ" ××‘×—×™× ×” ×˜×›× ×•×œ×•×’×™×ª ×™×›×œ×” ×œ×”×©××™×¨ ××ª ×‘×™×ª ×”××œ××›×” ×××—×•×¨ ×›××©×¨ ×”×¡×˜× ×“×¨×˜ ×”×ª×¢×©×™×™×ª×™ ×”×©×ª× ×”.

×›×™×•×, ×× ×”×œ×™× ×‘×¢×•×œ× ×”×‘×™× ×” ×”××œ××›×•×ª×™×ª ×¢×•××“×™× ×‘×¤× ×™ ×“×™×œ××•×ª ××¤×ª×™×¢×•×ª ×‘××•×¤×Ÿ ××“×”×™×. ×œ× ××“×•×‘×¨ ×‘×× ×•×¢×™ ×§×™×˜×•×¨, ××œ× ×‘××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× -- \term{LLM}. ×•×›××• ××•×ª× ×™×–××™× ××”×××” ×”-19, ×’× ×× ×”×œ×™ ×”×¢×™×“×Ÿ ×”×“×™×’×™×˜×œ×™ ×—×™×™×‘×™× ×œ×‘×—×•×¨ ×‘×—×•×›××”, ×›×™ ×”×”×©×œ×›×•×ª ×©×œ ×‘×—×™×¨×” ××•×˜×¢×™×ª ×™×›×•×œ×•×ª ×œ×”×™×•×ª ×›×‘×“×•×ª: ×¢×œ×•×™×•×ª ××•×¤×¨×–×•×ª, ×‘×™×¦×•×¢×™× × ××•×›×™×, ×ª×œ×•×ª ×‘×œ×ª×™ ×¨×¦×•×™×” ×‘×¡×¤×§ ×™×—×™×“, ××• ×’×¨×•×¢ ××›×œ -- ×”×¦×•×¨×š ×œ×”×ª×—×™×œ ××—×“×© ×›××©×¨ ×”×˜×›× ×•×œ×•×’×™×” ×©×‘×—×¨× ×• ×”×•×¤×›×ª ×œ××™×•×©× ×ª.

×‘×¤×¨×§ ×–×” × ×¢×¡×•×§ ×‘×©××œ×•×ª ×”××¡×˜×¨×˜×’×™×•×ª ×”××•×¨×›×‘×•×ª ×‘×™×•×ª×¨ ×©×œ ×”×˜××¢×ª AI ×‘××¨×’×•×Ÿ: ××™×–×” ××•×“×œ ×©×¤×” ×œ×‘×—×•×¨? ××™×š ×œ× ×”×œ ××ª ×–×™×›×¨×•×Ÿ ×”×©×™×—×”? ××ª×™ ×›×“××™ ×œ×”×©×§×™×¢ ×‘××•×“×œ Task-Specific ×•××ª×™ ×œ×”×™×©××¨ ×¢× General-Purpose? ×•×›×™×¦×“ ×œ×‘× ×•×ª ××¨×›×™×˜×§×˜×•×¨×” ×©×œ× ×ª××œ×¥ ××•×ª× ×• "×œ×”×ª×—×ª×Ÿ" ×¢× ×¡×¤×§ ××—×“ ×œ×©××¨×™×ª ×™××™ ×”×¤×¨×•×™×§×˜?

\section{×‘×—×™×¨×ª LLM: ×”××¤×” ×”××¡×˜×¨×˜×’×™×ª}

\subsection{×§×¨×™×˜×¨×™×•× ×™× ××¨×›×–×™×™× ×œ×‘×—×™×¨×ª ××•×“×œ}

×›××©×¨ ××¨×’×•×Ÿ ×¢×•××“ ×‘×¤× ×™ ×”×—×œ×˜×” ×¢×œ ×‘×—×™×¨×ª \term{LLM}, ×”×•× ×œ××¢×©×” ××§×‘×œ ×”×—×œ×˜×” ××¡×˜×¨×˜×’×™×ª ×¨×‘-×©× ×ª×™×ª. ×”×”×—×œ×˜×” ×”×–×• ×“×•××” ×™×•×ª×¨ ×œ×‘×—×™×¨×ª ×¡×¤×§ ERP ×××©×¨ ×œ×¨×›×™×©×ª ×ª×•×›× ×” ×¤×©×•×˜×”. ×”×¡×™×‘×” ×¤×©×•×˜×”: ×›×œ ×‘×—×™×¨×ª ××•×“×œ ××’×™×¢×” ×¢× ×”×©×œ×›×•×ª ×¢××•×§×•×ª ×¢×œ ×”××¨×›×™×˜×§×˜×•×¨×”, ×¢×œ ×”× ×ª×•× ×™×, ×•×¢×œ ×”×¦×•×•×ª ×©×™×¢×‘×•×“ ××•×œ×”.

\begin{notebox}[×©××œ×•×ª ×”××¤×ª×— ×œ×¤× ×™ ×‘×—×™×¨×ª LLM]
×œ×¤× ×™ ×©××ª× ××ª×—×™×œ×™× ×œ×”×©×•×•×ª ×¦×™×•× ×™× ×‘-\term{Benchmarks}, ×©××œ×• ××ª ×¢×¦××›×:
\begin{enumerate}
  \item ××”×Ÿ ×”××©×™××•×ª ×”×¡×¤×¦×™×¤×™×•×ª ×©×”××•×“×œ ×¦×¨×™×š ×œ×‘×¦×¢? (×¡×™×›×•×, ×™×¦×™×¨×ª ×ª×•×›×Ÿ, ×§×•×“, ×©×™×—×”?)
  \item ××”×™ ×¨××ª ×”×¨×’×™×©×•×ª ×©×œ ×”× ×ª×•× ×™×? (×”×× × ×ª×•× ×™× ×¨×’×™×©×™× ×™×›×•×œ×™× ×œ×¢×–×•×‘ ××ª ×”××¨×’×•×Ÿ?)
  \item ××”×™ ×”×ª×§×¦×™×‘ ×”×—×•×“×©×™ ×”××•×§×¦×”? (×¢×œ×•×ª ×œ×˜×•×§×Ÿ Ã— × ×¤×— ×©×™××•×© ×—×–×•×™)
  \item ××”×™ ×¨××ª ×”-\term{Latency} ×”××§×¡×™××œ×™×ª ×”××§×•×‘×œ×ª? (×–××Ÿ ×ª×’×•×‘×”)
  \item ×”×× × ×“×¨×©×ª ×ª××™×›×” ×‘×©×¤×•×ª ××¨×•×‘×•×ª? (××¢×‘×¨ ×œ×× ×’×œ×™×ª)
  \item ××”×• ×’×•×“×œ \term{Context Window} ×”× ×“×¨×©? (×›××” ××™×“×¢ ×¦×¨×™×š ×œ×¢×‘×“ ×‘×• ×–×× ×™×ª)
\end{enumerate}
\end{notebox}

×‘×•××• × × ×ª×— ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ ×‘× ×¤×¨×“.

\subsubsection{×‘×™×¦×•×¢×™× ×œ×¤×™ ×¡×•×’ ××©×™××”}

×œ× ×›×œ ××•×“×œ ××¦×˜×™×™×Ÿ ×‘×›×œ ××©×™××”~\cite{EnterpriseLLMBenchmark2025,LLMPromptScope2025}. \term{GPT-4}, ×œ××©×œ, ××¦×˜×™×™×Ÿ ×‘××©×™××•×ª ×©×“×•×¨×©×•×ª ×”×‘× ×” ×¢××•×§×” ×•×”×™×’×™×•×Ÿ ××•×¨×›×‘, ××š ×”×•× ×™×§×¨ ×™×—×¡×™×ª ×•××™×˜×™. \term{Claude 3.5 Sonnet}, ×œ×¢×•××ª ×–××ª, ××¦×™×’ ×™×›×•×œ×•×ª ××¦×•×™× ×•×ª ×‘×›×ª×™×‘×” ×™×¦×™×¨×ª×™×ª ×•× ×™×ª×•×— ×˜×§×¡×˜ ××¨×•×š, ×ª×•×š ×©×”×•× ××”×™×¨ ×™×•×ª×¨ ×•×œ×¢×ª×™× ×–×•×œ ×™×•×ª×¨. \term{GPT-3.5 Turbo} ×”×•× ×–×•×œ ×‘××™×•×—×“ ×•××”×™×¨, ××š ×¤×—×•×ª ××“×•×™×§ ×‘××©×™××•×ª ××•×¨×›×‘×•×ª.

\begin{examplebox}[×“×•×’××”: ×—×‘×¨×ª ×‘×™×˜×•×— ×‘×•×—×¨×ª ××•×“×œ]
×—×‘×¨×ª ×‘×™×˜×•×— ×’×“×•×œ×” ×¦×¨×™×›×” AI ×œ×©×ª×™ ××˜×¨×•×ª ×©×•× ×•×ª:
\begin{enumerate}
  \item \textbf{×ª××™×›×ª ×œ×§×•×—×•×ª ×‘×–××Ÿ ×××ª} -- ×ª×’×•×‘×•×ª ××”×™×¨×•×ª ×œ×©××œ×•×ª × ×¤×•×¦×•×ª.
  \item \textbf{× ×™×ª×•×— ×ª×‘×™×¢×•×ª ××•×¨×›×‘×•×ª} -- ×‘×“×™×§×” ×¢××•×§×” ×©×œ ××¡××›×™× ×•×–×™×”×•×™ ×”×•× ××•×ª.
\end{enumerate}

××¡×˜×¨×˜×’×™×” ×—×›××” ×”×™× ×œ×”×©×ª××© ×‘\textbf{-Multi-Model Strategy}:
\begin{itemize}
  \item ×¢×‘×•×¨ ×ª××™×›×ª ×œ×§×•×—×•×ª: \term{GPT-3.5 Turbo} (××”×™×¨, ×–×•×œ, ××¡×¤×™×§ ×˜×•×‘)
  \item ×¢×‘×•×¨ × ×™×ª×•×— ×ª×‘×™×¢×•×ª: \term{GPT-4} ××• \term{Claude Opus} (××™×˜×™ ××š ××“×•×™×§)
\end{itemize}

×”×ª×•×¦××”: ×—×™×¡×›×•×Ÿ ×©×œ 70\% ×‘×¢×œ×•×™×•×ª API ×ª×•×š ×©××™×¨×” ×¢×œ ××™×›×•×ª ×’×‘×•×”×” ×‘××©×™××•×ª ×§×¨×™×˜×™×•×ª.
\end{examplebox}

\subsubsection{×¨×’×™×©×•×ª × ×ª×•× ×™× ×•×¤×¨×˜×™×•×ª}

×× ×”××¨×’×•×Ÿ ××¢×‘×“ ××™×“×¢ ×¨×’×™×© -- ×¨×¤×•××™, ×¤×™× × ×¡×™, ××™×©×™ -- ×”×©××œ×” "×œ××Ÿ ×”×•×œ×›×™× ×”× ×ª×•× ×™× ×©×œ×™?" ×”×•×¤×›×ª ×§×¨×™×˜×™×ª. ××•×“×œ×™× ×‘×¢× ×Ÿ ×›××• \term{GPT-4} ××• \term{Claude} ×©×•×œ×—×™× ××ª ×”× ×ª×•× ×™× ×œ×©×¨×ª×™ ×”×¡×¤×§. ××× × OpenAI ×•-Anthropic ××‘×˜×™×—×™× ×©× ×ª×•× ×™× ×œ× ××©××©×™× ×œ××™××•×Ÿ × ×•×¡×£, ××š ×¢×‘×•×¨ ××¨×’×•× ×™× ××¡×•×™××™× ××¤×™×œ×• ×–×” ×œ× ××¡×¤×™×§ ×˜×•×‘.

×‘×ª×¨×—×™×©×™× ××œ×•, ×¤×ª×¨×•× ×•×ª \term{Self-Hosted} ×›××• \term{Llama 3.1 405B} ××• \term{Mistral Large} ×××¤×©×¨×™× ×”×¨×¦×” ××œ××” On-Premises, ×›×š ×©×”× ×ª×•× ×™× ×œ×¢×•×œ× ×œ× ×¢×•×–×‘×™× ××ª ×”××¨×’×•×Ÿ. ××× × ×™×© ×¢×œ×•×ª ××™× ×¤×¨×¡×˜×¨×•×§×˜×•×¨×” (×©×¨×ª×™×, GPU), ××š ×œ×¢×ª×™× ×–×” ×”××—×™×¨ ×”×”×›×¨×—×™ ×©×œ ×¤×¨×˜×™×•×ª.

\begin{formulabox}[× ×•×¡×—×ª ×¢×œ×•×ª ×¤×¨×˜×™×•×ª]
\[
\text{Total Privacy Cost} = \text{Infrastructure Cost} + \text{Maintenance} + \text{HR Cost}
\]
×œ×¢×•××ª:
\[
\text{Cloud API Cost} = \text{Tokens} \times \text{Price per Token}
\]
\textbf{× ×§×•×“×ª ×”××™×–×•×Ÿ} ×”×™× ×”× ×¤×— ×”×—×•×“×©×™ ×©×‘×• ×©× ×™ ×”××¡×œ×•×œ×™× ×©×•×•×™× ×‘×¢×œ×•×ª.
\end{formulabox}

\subsubsection{×’×•×“×œ Context Window}

\term{Context Window} ×”×•× ××¡×¤×¨ ×”×˜×•×§× ×™× ×”××§×¡×™××œ×™ ×©×”××•×“×œ ×™×›×•×œ ×œ×¢×‘×“ ×‘×©×™×—×” ××—×ª. ×–×” ×›×•×œ×œ ××ª ×›×œ ×”×”×™×¡×˜×•×¨×™×” ×©×œ ×”×©×™×—×”, ××ª ×”×¤×¨×•××¤×˜, ×•××ª ×”×ª×©×•×‘×” ×”×¦×¤×•×™×”.

\begin{itemize}
  \item \textbf{GPT-3.5 Turbo}: 16K ×˜×•×§× ×™× (×›-12,000 ××™×œ×™×)
  \item \textbf{GPT-4 Turbo}: 128K ×˜×•×§× ×™× (×›-96,000 ××™×œ×™×)
  \item \textbf{Claude 3.5 Sonnet}: 200K ×˜×•×§× ×™× (×›-150,000 ××™×œ×™×)
  \item \textbf{Gemini 1.5 Pro}: 2M ×˜×•×§× ×™× (×›-1.5 ××™×œ×™×•×Ÿ ××™×œ×™×!)
\end{itemize}

×œ××” ×–×” ×—×©×•×‘? ×× ××ª× ×¦×¨×™×›×™× ×œ×¢×‘×“ ××¡××›×™× ××¨×•×›×™× (×—×•×–×™×, ×“×•×—×•×ª ×©× ×ª×™×™×, ×ª×™×¢×•×“ ×˜×›× ×™), Context Window ×’×“×•×œ ×—×•×¡×š ××ª ×”×¦×•×¨×š ×‘-\term{RAG} ×•×‘×¢×™×‘×•×“ ×¨×‘-×©×œ×‘×™.

\begin{examplebox}[×“×•×’××”: ××©×¨×“ ×¢×•×¨×›×™ ×“×™×Ÿ ×× ×ª×— ×—×•×–×™×]
××©×¨×“ ×¢×•×¨×›×™ ×“×™×Ÿ ×¦×¨×™×š ×œ× ×ª×— ×—×•×–×™× ××•×¨×›×‘×™× ×‘××•×¨×š 50,000 ××™×œ×™×. ×× ×”× ××©×ª××©×™× ×‘-GPT-3.5 Turbo, ×”× ×—×™×™×‘×™× ×œ×¤×¦×œ ××ª ×”×—×•×–×” ×œ×—×œ×§×™× ×§×˜× ×™× ×•×œ×©×œ×•×— ×›×œ ×—×œ×§ ×‘× ×¤×¨×“ -- ×–×” ×™×•×¦×¨ ×¤×™×¦×•×œ ×”×§×©×¨ ×•×¡×™×›×•×Ÿ ×œ×”×—××¦×ª ×§×©×¨×™× ×‘×™×Ÿ ×¡×¢×™×¤×™×.

×¤×ª×¨×•×Ÿ: ×©×™××•×© ×‘-\textbf{Claude 3.5 Sonnet} ×¢× 200K Context Window ×××¤×©×¨ ×œ×©×œ×•×— ××ª ×›×œ ×”×—×•×–×” ×‘×‘×ª ××—×ª, ×•×›×š ×”××•×“×œ ×¨×•××” ××ª ×”×ª××•× ×” ×”××œ××”.

\textbf{×¢×œ×•×ª ×œ×¤×™ ××•×“×œ (×œ×—×•×–×” ×‘×•×“×“ ×©×œ 50K ××™×œ×™× = 67K ×˜×•×§× ×™×):}
\begin{itemize}
  \item GPT-3.5 Turbo: ×¤×™×¦×•×œ ×œ-5 ×§×¨×™××•×ª Ã— \$0.002/1K = \$0.67 (××š ××™×›×•×ª × ××•×›×” ×™×•×ª×¨)
  \item Claude 3.5 Sonnet: ×§×¨×™××” ××—×ª Ã— \$0.003/1K = \$0.20 (××™×›×•×ª ×’×‘×•×”×” ×™×•×ª×¨)
\end{itemize}
\end{examplebox}

\subsection{××“×“×™ ×”×©×•×•××”: Performance vs Cost}

××—×ª ×”×“×¨×›×™× ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×”×©×•×•×ª ××•×“×œ×™× ×”×™× ×‘×××¦×¢×•×ª \textbf{Performance-to-Cost Ratio}~\cite{LLMPricing2025,LLMCostOptimization2025}. ×˜×‘×œ×”~\ref{tab:llm-comparison} ××¦×™×’×” ×”×©×•×•××” ×‘×™×Ÿ ×”××•×“×œ×™× ×”××•×‘×™×œ×™×, ×•××™×•×¨~\ref{fig:performance-cost} ××“×’×™× ××ª ×”×™×—×¡ ×‘×™×Ÿ ×‘×™×¦×•×¢×™× ×œ×¢×œ×•×ª.

\begin{formulabox}[×™×—×¡ ×‘×™×¦×•×¢×™× ×œ×¢×œ×•×ª]
\[
\text{Performance/Cost Ratio} = \frac{\text{Performance Score}}{\text{Monthly Cost}}
\]
×›××©×¨:
\begin{itemize}
  \item \textbf{Performance Score}: ×¦×™×•×Ÿ ×× ×•×¨××œ (0-100) ×××“×“×™× ×›××• MMLU, HumanEval, ××• ×‘×“×™×§×” ×¤× ×™××™×ª
  \item \textbf{Monthly Cost}: ×¢×œ×•×ª ×—×•×“×©×™×ª ××©×•×¢×¨×ª ×œ×¤×™ × ×¤×— ×©×™××•×©
\end{itemize}
\end{formulabox}

\begin{table}[H]
\centering
\caption{×”×©×•×•××ª ××•×“×œ×™× ××•×‘×™×œ×™× (2025)}
\label{tab:llm-comparison}
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{××•×“×œ} & \textbf{MMLU} & \textbf{×¢×œ×•×ª/1M ×˜×•×§×Ÿ} & \textbf{Context} & \textbf{Latency} \\
\midrule
GPT-4 Turbo & 86.4 & \$10 & 128K & 3-5s \\
Claude 3.5 Sonnet & 88.7 & \$3 & 200K & 2-4s \\
GPT-3.5 Turbo & 70.0 & \$0.50 & 16K & 0.5-1s \\
Gemini 1.5 Pro & 85.9 & \$7 & 2M & 4-6s \\
Llama 3.1 70B & 79.3 & \textenglish{Self-hosted} & 128K & 1-2s \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\begin{english}
\begin{tikzpicture}[scale=1.2]
  % Axes
  \draw[->] (0,0) -- (10,0) node[right] {×¢×œ×•×ª (\$ ×œ××™×œ×™×•×Ÿ ×˜×•×§× ×™×)};
  \draw[->] (0,0) -- (0,8) node[above] {×‘×™×¦×•×¢×™× (MMLU)};

  % Grid
  \draw[gray!30, dashed] (0,2) -- (10,2);
  \draw[gray!30, dashed] (0,4) -- (10,4);
  \draw[gray!30, dashed] (0,6) -- (10,6);
  \draw[gray!30, dashed] (2,0) -- (2,8);
  \draw[gray!30, dashed] (4,0) -- (4,8);
  \draw[gray!30, dashed] (6,0) -- (6,8);
  \draw[gray!30, dashed] (8,0) -- (8,8);

  % Y-axis labels (Performance)
  \node[left] at (0,2) {70};
  \node[left] at (0,4) {80};
  \node[left] at (0,6) {90};

  % X-axis labels (Cost)
  \node[below] at (0.5,0) {0};
  \node[below] at (2.5,0) {2};
  \node[below] at (5,0) {5};
  \node[below] at (7.5,0) {10};

  % Data points
  % GPT-3.5 Turbo: Cost=0.5, Performance=70
  \fill[blue] (0.5,2) circle (3pt);
  \node[above right, blue] at (0.5,2) {\small GPT-3.5};

  % Claude 3.5 Sonnet: Cost=3, Performance=88.7
  \fill[green!60!black] (3,6.8) circle (3pt);
  \node[above, green!60!black] at (3,6.8) {\small Claude 3.5};

  % GPT-4 Turbo: Cost=10, Performance=86.4
  \fill[red] (9,6.2) circle (3pt);
  \node[left, red] at (9,6.2) {\small GPT-4};

  % Gemini 1.5 Pro: Cost=7, Performance=85.9
  \fill[orange] (6.5,6) circle (3pt);
  \node[below, orange] at (6.5,6) {\small Gemini};

  % Pareto frontier (efficient models)
  \draw[thick, dashed, purple] (0.5,2) -- (3,6.8);

  % Legend
  \node[purple, above right] at (0.5,6.5) {×—×–×™×ª ×¤××¨×˜×•};
  \node[purple, above right] at (0.5,6) {\small (××•×“×œ×™× ×™×¢×™×œ×™×)};
\end{tikzpicture}
\end{english}
\caption{×’×¨×£ Scatter: ×‘×™×¦×•×¢×™× ××•×œ ×¢×œ×•×ª}
\label{fig:performance-cost}
\end{figure}

×”×’×¨×£ ×œ×¢×™×œ ××“×’×™× × ×§×•×“×” ×—×©×•×‘×”: \textbf{Claude 3.5 Sonnet} × ××¦× ×¢×œ "×—×–×™×ª ×¤××¨×˜×•" -- ×”×•× ××¦×™×¢ ×©×™×œ×•×‘ ××•×¤×˜×™××œ×™ ×©×œ ×‘×™×¦×•×¢×™× ×•×¢×œ×•×ª. ××•×“×œ×™× ×©××¢×œ ×”×§×• ×”×¡×’×•×œ (×›××• GPT-4) ×™×§×¨×™× ×™×•×ª×¨ ×‘×œ×™ ×œ×ª×ª ×©×™×¤×•×¨ ×¤×¨×•×¤×•×¨×¦×™×•× ×œ×™, ×•××•×“×œ×™× ××ª×—×ª ×œ×§×• (×›××• GPT-3.5) ×–×•×œ×™× ××š ×¤×—×•×ª ××“×•×™×§×™×.

\subsection{Decision Tree ×œ×‘×—×™×¨×ª LLM}

××™×•×¨~\ref{fig:decision-tree} ××¦×™×’ ×¢×¥ ×”×—×œ×˜×•×ª ×¤×©×•×˜ ×œ×‘×—×™×¨×ª \en{LLM} ×¢×œ ×‘×¡×™×¡ ×¦×¨×›×™ ×”××¨×’×•×Ÿ.

\begin{figure}[H]
\centering
\begin{english}
\begin{tikzpicture}[
  node distance=1.5cm and 2cm,
  decision/.style={diamond, draw, fill=yellow!20, text width=4cm, text centered, inner sep=2pt},
  outcome/.style={rectangle, draw, fill=green!20, text width=3cm, text centered, rounded corners, minimum height=1cm},
  arrow/.style={->, thick}
]

% Root
\node[decision] (root) {×”×× × ×ª×•× ×™× ×¨×’×™×©×™×?};

% Level 1
\node[decision, below left=of root] (cloud) {×”×× × ×“×¨×©\\Context ×’×“×•×œ?};
\node[outcome, below right=of root] (selfhosted) {×©×§×•×œ\\Llama 3.1\\××• Mistral\\(Self-Hosted)};

% Level 2
\node[decision, below left=of cloud] (budget) {×ª×§×¦×™×‘\\×’×‘×•×”?};
\node[outcome, below right=of cloud] (gemini) {Gemini 1.5 Pro\\(2M Context)};

% Level 3
\node[outcome, below left=of budget] (gpt35) {GPT-3.5 Turbo\\(×–×•×œ ×•××”×™×¨)};
\node[outcome, below right=of budget] (claude) {Claude 3.5\\××• GPT-4};

% Arrows
\draw[arrow] (root) -- node[left, pos=0.3] {\small ×œ×} (cloud);
\draw[arrow] (root) -- node[right, pos=0.3] {\small ×›×Ÿ} (selfhosted);
\draw[arrow] (cloud) -- node[left, pos=0.3] {\small ×œ×} (budget);
\draw[arrow] (cloud) -- node[right, pos=0.3] {\small ×›×Ÿ} (gemini);
\draw[arrow] (budget) -- node[left, pos=0.3] {\small ×œ×} (gpt35);
\draw[arrow] (budget) -- node[right, pos=0.3] {\small ×›×Ÿ} (claude);

\end{tikzpicture}
\end{english}
\caption{Decision Tree ×œ×‘×—×™×¨×ª LLM}
\label{fig:decision-tree}
\end{figure}

\section{×‘×—×™×¨×ª Embedding Models: Task-Specific ××• General?}

\term{Embedding Models} ×”× ×”×× ×•×¢ ×©×××—×•×¨×™ ××¢×¨×›×•×ª \term{RAG} ×•×—×™×¤×•×© ×¡×× ×˜×™~\cite{MTEBReview2024,DomainSpecificEmbedding2024}. ×”× ×”×•×¤×›×™× ×˜×§×¡×˜ ×œ×•×•×§×˜×•×¨×™× ××¡×¤×¨×™×™×, ××” ×©×××¤×©×¨ ×œ××“×•×“ "×“××™×•×Ÿ" ×‘×™×Ÿ ××©×¤×˜×™×, ××¡××›×™× ××• ×©××œ×•×ª.

\subsection{General-Purpose Embeddings}

××•×“×œ×™× ×›××• \term{text-embedding-3-large} ×©×œ OpenAI ××• \term{NV-Embed-v2} ×©×œ NVIDIA ×”× "××•×“×œ×™× ×›×œ×œ×™×™×" -- ×××•×× ×™× ×¢×œ ××’×•×•×Ÿ ×¨×—×‘ ×©×œ ×˜×§×¡×˜×™× ×•××¡×•×’×œ×™× ×œ×˜×¤×œ ×‘×¨×•×‘ ×”×ª×—×•××™× ×‘×¦×•×¨×” ×¡×‘×™×¨×”.

\textbf{×™×ª×¨×•× ×•×ª:}
\begin{itemize}
  \item ×¤×©×•×˜ ×œ×™×™×©×•× -- ××™×Ÿ ×¦×•×¨×š ×‘××™××•×Ÿ × ×•×¡×£
  \item ××ª××™× ×œ×¨×•×‘ ×”××§×¨×™× (80\% ××”××©×™××•×ª)
  \item ××¦×˜×™×™×Ÿ ×‘×˜×§×¡×˜×™× ×›×œ×œ×™×™× ×•×‘×©×¤×” ×˜×‘×¢×™×ª
\end{itemize}

\textbf{×—×¡×¨×•× ×•×ª:}
\begin{itemize}
  \item ×¤×—×•×ª ××“×•×™×§ ×‘×ª×—×•××™× ×××•×“ ×¡×¤×¦×™×¤×™×™× (×¨×¤×•××”, ××©×¤×˜×™×, ×›×™××™×”)
  \item ×¢×œ×•×™×•×ª API ××ª××©×›×•×ª (×× ××©×ª××©×™× ×‘-API ×—×™×¦×•× ×™)
\end{itemize}

\subsection{Task-Specific Embeddings}

×‘××§×¨×™× ×©×‘×”× ×”×˜×§×¡×˜ ×©×œ×›× ×××•×“ ×¡×¤×¦×™×¤×™ -- ×œ××©×œ, ××¡××›×™× ×¨×¤×•××™×™×, ×ª×§× ×•×ª ××©×¤×˜×™×•×ª, ××• ×§×•×“ -- ×›×“××™ ×œ×©×§×•×œ \term{Fine-Tuning} ×©×œ ××•×“×œ Embedding ××• ×©×™××•×© ×‘××•×“×œ ×™×™×¢×•×“×™.

\begin{examplebox}[×“×•×’××”: ×—×‘×¨×ª ×ª×¨×•×¤×•×ª ××¤×ª×—×ª Embedding ×™×™×¢×•×“×™]
×—×‘×¨×ª ×ª×¨×•×¤×•×ª ×’×“×•×œ×” ×”×©×ª××©×” ×‘-\term{text-embedding-3-large} ×œ××—×–×•×¨ ××××¨×™× ××“×¢×™×™×, ××š ×’×™×œ×ª×” ×©×”×“×™×•×§ × ××•×š -- ×”××•×“×œ ×œ× ×”×‘×™×Ÿ ×˜×•×‘ ××•× ×—×™× ×¨×¤×•××™×™× ×›××• "nephrotoxicity" ××• "cytochrome P450".

×”×¤×ª×¨×•×Ÿ: \textbf{Fine-Tune} ×©×œ ××•×“×œ \term{BGE-M3} ×¢×œ 50,000 ××××¨×™× ×¨×¤×•××™×™×.

\textbf{×ª×•×¦××”:}
\begin{itemize}
  \item ×“×™×•×§ ××—×–×•×¨ ×¢×œ×” ×-65\% ×œ-89\%
  \item ×¢×œ×•×ª ×—×“-×¤×¢××™×ª: \$5,000 (××™××•×Ÿ)
  \item ×¢×œ×•×ª ×©×•×˜×¤×ª: \$200/×—×•×“×© (Self-Hosted)
  \item ×œ×¢×•××ª: \$2,000/×—×•×“×© (API ×©×œ OpenAI)
\end{itemize}

× ×§×•×“×ª ××™×–×•×Ÿ: 2.5 ×—×•×“×©×™×.
\end{examplebox}

\section{×‘×—×™×¨×ª Database: Vector, Relational, ××• Hybrid?}

×›××©×¨ ×‘×•× ×™× ××¢×¨×›×ª AI, ××—×ª ×”×©××œ×•×ª ×”×§×¨×™×˜×™×•×ª ×”×™×: ××™×¤×” ×œ××—×¡×Ÿ ××ª ×”× ×ª×•× ×™×?

\subsection{Vector Databases}

\term{Vector Databases} ×›××• \term{Pinecone}, \term{Weaviate}, ×•-\term{Qdrant} ××•×ª×××™× ×œ××—×¡×•×Ÿ ×•×—×™×¤×•×© ×©×œ Embeddings~\cite{QdrantBenchmarks2024,VectorDBComparison2025}. ×”× ×××¤×©×¨×™× ×—×™×¤×•×© ×œ×¤×™ ×“××™×•×Ÿ (\term{Similarity Search}) ×‘××”×™×¨×•×ª ×’×‘×•×”×”.

\textbf{××ª×™ ×œ×”×©×ª××©:}
\begin{itemize}
  \item ×›××©×¨ ×¨×•×‘ ×”×©××™×œ×ª×•×ª ×”×Ÿ ×¡×× ×˜×™×•×ª ("××¦× ××¡××›×™× ×“×•××™× ×œ×–×”")
  \item ×›××©×¨ ×™×© ×¦×•×¨×š ×‘-RAG
  \item ×›××©×¨ ×”× ×ª×•× ×™× ×”× ×œ× ××•×‘× ×™× (×˜×§×¡×˜ ×—×•×¤×©×™, ×ª××•× ×•×ª)
\end{itemize}

\subsection{Relational Databases}

××¡×“×™ × ×ª×•× ×™× ×™×—×¡×™×™× ××¡×•×¨×ª×™×™× ×›××• \term{PostgreSQL} ××• \term{MySQL} ×˜×•×‘×™× ×œ××™×“×¢ ××•×‘× ×”: ×˜×‘×œ××•×ª, ×©×•×¨×•×ª, ×¢××•×“×•×ª.

\textbf{××ª×™ ×œ×”×©×ª××©:}
\begin{itemize}
  \item ×›××©×¨ ×”× ×ª×•× ×™× ××•×‘× ×™× (×œ×§×•×—×•×ª, ×”×–×× ×•×ª, ××œ××™)
  \item ×›××©×¨ ×™×© ×¦×•×¨×š ×‘-\term{ACID Transactions} (×¢×¡×§××•×ª ××˜×•××™×•×ª)
  \item ×›××©×¨ ×”×©××™×œ×ª×•×ª ×”×Ÿ ××“×•×™×§×•×ª (WHERE, JOIN)
\end{itemize}

\subsection{Hybrid Approach}

×’×™×©×” ×”×™×‘×¨×™×“×™×ª ××©×œ×‘×ª ××ª ×©× ×™ ×”×¢×•×œ××•×ª: × ×ª×•× ×™× ××•×‘× ×™× ×‘-SQL, Embeddings ×‘-Vector DB. ×˜×‘×œ×”~\ref{tab:db-comparison} ××¡×›××ª ××ª ×”×”×‘×“×œ×™× ×‘×™×Ÿ ×”×’×™×©×•×ª ×”×©×•× ×•×ª.

\begin{examplebox}[×“×•×’××”: ××ª×¨ e-commerce ×¢× AI]
××ª×¨ ××¡×—×¨ ××œ×§×˜×¨×•× ×™ ×¨×•×¦×” ×œ×”×¦×™×¢ ×”××œ×¦×•×ª ××•×ª×××•×ª ××™×©×™×ª.

\textbf{××¨×›×™×˜×§×˜×•×¨×” ×”×™×‘×¨×™×“×™×ª:}
\begin{itemize}
  \item \textbf{PostgreSQL}: ×¤×¨×˜×™ ×œ×§×•×—×•×ª, ×”×–×× ×•×ª, ××œ××™ (× ×ª×•× ×™× ××•×‘× ×™×)
  \item \textbf{Pinecone}: Embeddings ×©×œ ×ª×™××•×¨×™ ××•×¦×¨×™× ×•×”×™×¡×˜×•×¨×™×™×ª ×’×œ×™×©×” (×—×™×¤×•×© ×¡×× ×˜×™)
\end{itemize}

\textbf{×ª×”×œ×™×š ×”××œ×¦×”:}
\begin{enumerate}
  \item ×©×œ×•×£ ×-PostgreSQL: ×”×™×¡×˜×•×¨×™×™×ª ×¨×›×™×©×•×ª ×©×œ ×”×œ×§×•×—
  \item ×”××¨ ××ª ×–×” ×œ-Embedding ×•×—×¤×© ×‘-Pinecone: ××•×¦×¨×™× ×“×•××™×
  \item ×©×œ×•×£ ×-PostgreSQL: ×¤×¨×˜×™ ×”××•×¦×¨×™× ×©× ××¦××• (××—×™×¨, ××œ××™)
  \item ×”×¦×’ ×œ×œ×§×•×—
\end{enumerate}
\end{examplebox}

\begin{table}[H]
\centering
\caption{×”×©×•×•××ª ×¡×•×’×™ Databases}
\label{tab:db-comparison}
\begin{tabular}{@{}lp{4cm}p{4cm}p{4cm}@{}}
\toprule
\textbf{×§×¨×™×˜×¨×™×•×Ÿ} & \textbf{Vector DB} & \textbf{Relational DB} & \textbf{Hybrid} \\
\midrule
×—×™×¤×•×© ×¡×× ×˜×™ & ××¦×•×™×Ÿ & ×œ× × ×ª××š & ××¦×•×™×Ÿ \\
×©××™×œ×ª×•×ª SQL & ×œ× × ×ª××š & ××¦×•×™×Ÿ & ××¦×•×™×Ÿ \\
××•×¨×›×‘×•×ª ×”×˜××¢×” & ×‘×™× ×•× ×™×ª & × ××•×›×” & ×’×‘×•×”×” \\
×¢×œ×•×ª & ×‘×™× ×•× ×™×ª-×’×‘×•×”×” & × ××•×›×” & ×’×‘×•×”×” \\
××”×™×¨×•×ª (Similarity) & ××”×™×¨×” ×××•×“ & ××™×˜×™×ª & ××”×™×¨×” \\
\bottomrule
\end{tabular}
\end{table}

\section{× ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×‘-LLM: Short-term, Long-term, External}

××—×“ ×”××ª×’×¨×™× ×”×’×“×•×œ×™× ×‘×™×•×ª×¨ ×‘×¢×‘×•×“×” ×¢× \term{LLM} ×”×•× × ×™×”×•×œ \textbf{×–×™×›×¨×•×Ÿ ×”×©×™×—×”}~\cite{RecursiveSummarization2023,CognitiveMemoryLLM2025}. ×‘× ×™×’×•×“ ×œ×‘× ×™ ××“×, ×©×–×•×›×¨×™× ××™× ×¡×•×£ ×©×™×—×•×ª ×§×•×“××•×ª, LLM "×©×•×›×—" ×”×›×œ ×‘×¨×’×¢ ×©×”×©×™×—×” ××¡×ª×™×™××ª. ×‘× ×•×¡×£, ×’× ×‘×ª×•×š ×©×™×—×” ××—×ª, ×™×© ××’×‘×œ×” ×¢×œ ×›××•×ª ×”××™×“×¢ ×©×”×•× ×™×›×•×œ "×œ×–×›×•×¨" -- ×–×” × ×§×¨× \term{Context Window}.

\subsection{Short-term Memory: × ×™×”×•×œ ×”×©×™×—×” ×”× ×•×›×—×™×ª}

\textbf{Short-term Memory} ×”×™× ×”×”×™×¡×˜×•×¨×™×” ×”××™×™×“×™×ª ×©×œ ×”×©×™×—×” ×”× ×•×›×—×™×ª. ×‘×›×œ ×¤×¢× ×©××ª× ×©×•×œ×—×™× ×”×•×“×¢×” ×œ-LLM, ××ª× ×‘×¢×¦× ×©×•×œ×—×™×:
\begin{enumerate}
  \item ××ª ×›×œ ×”×”×•×“×¢×•×ª ×”×§×•×“××•×ª ×‘×©×™×—×”
  \item ××ª ×”×”×•×“×¢×” ×”×—×“×©×”
\end{enumerate}

×–×” ××•××¨ ×©×›×›×œ ×©×”×©×™×—×” ××¨×•×›×” ×™×•×ª×¨, ×›×š ××ª× ××©×œ××™× ×™×•×ª×¨ -- ×›×™ ××ª× ××¢×œ×™× ×©×•×‘ ×•×©×•×‘ ××ª ×›×œ ×”×”×™×¡×˜×•×¨×™×”.

\begin{formulabox}[×¢×œ×•×ª ×©×™×—×” ×××•×©×›×ª]
×× ×›×œ ×”×•×“×¢×” ××•×¡×™×¤×” ×××•×¦×¢ ×©×œ 100 ×˜×•×§× ×™×, ×•×‘×©×™×—×” ×™×© 20 ×”×•×“×¢×•×ª:
\[
\text{Total Tokens} = 100 \times \frac{20 \times (20+1)}{2} = 21,000 \hebmath{ ×˜×•×§× ×™×}
\]
×–×” ×‘×’×œ×œ ×©×”×”×•×“×¢×” ×”-1 × ×©×œ×—×ª 20 ×¤×¢××™×, ×”×”×•×“×¢×” ×”-2 × ×©×œ×—×ª 19 ×¤×¢××™×, ×•×›×•'.
\end{formulabox}

\textbf{××¡×˜×¨×˜×’×™×•×ª ×œ×—×™×¡×›×•×Ÿ:}
\begin{itemize}
  \item \textbf{Truncation}: ×—×ª×•×š ×”×•×“×¢×•×ª ×™×©× ×•×ª ×›×©×”×Ÿ ×—×•×¨×’×•×ª ×-Context Window
  \item \textbf{Summarization}: ×¡×›× ×›×œ 10 ×”×•×“×¢×•×ª ×œ×¤×¡×§×” ××—×ª
  \item \textbf{Sliding Window}: ×©××•×¨ ×¨×§ ××ª ×”-N ×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª
\end{itemize}

\subsection{Long-term Memory: ×–×™×›×¨×•×Ÿ ×‘×™×Ÿ ×©×™×—×•×ª}

\textbf{Long-term Memory} ×”×•× ×”×™×›×•×œ×ª ×©×œ ×”××¢×¨×›×ª "×œ×–×›×•×¨" ××©×”×• ×’× ××—×¨×™ ×©×”×©×™×—×” ×”×¡×ª×™×™××”. ×œ××©×œ, ×× ×œ×§×•×— ×××¨ ×œ×š ×‘×©×‘×•×¢ ×©×¢×‘×¨ "×× ×™ ×¦××—×•× ×™", ××ª×” ×¨×•×¦×” ×©×”×¡×•×›×Ÿ ×™×“×¢ ××ª ×–×” ×’× ×‘×©×™×—×” ×”×‘××”.

\textbf{×“×¨×›×™× ×œ×™×™×©×•× Long-term Memory:}
\begin{enumerate}
  \item \textbf{Database ×©×œ ×¢×•×‘×“×•×ª}: ×©××•×¨ ×¢×•×‘×“×•×ª ×¢×œ ×”××©×ª××© (×©×, ×”×¢×“×¤×•×ª, ×”×™×¡×˜×•×¨×™×”) ×‘-SQL
  \item \textbf{Vector Database ×œ×©×™×—×•×ª ×§×•×“××•×ª}: ×©××•×¨ Embeddings ×©×œ ×›×œ ×©×™×—×”, ×•×›×©××ª×—×™×œ×” ×©×™×—×” ×—×“×©×” -- ××—×–×¨ ×©×™×—×•×ª ×¨×œ×•×•× ×˜×™×•×ª
  \item \textbf{Summarized History}: ×¡×›× ××ª ×›×œ ×”×©×™×—×•×ª ×”×§×•×“××•×ª ×œ-"×ª×§×¦×™×¨ ××©×ª××©" (2-3 ×¤×¡×§××•×ª)
\end{enumerate}

\begin{examplebox}[×“×•×’××”: ×¡×•×›×Ÿ ×ª××™×›×” ×¢× ×–×™×›×¨×•×Ÿ ××¨×•×š ×˜×•×•×—]
×¡×˜××¨×˜××¤ ×‘×•× ×” ×¡×•×›×Ÿ ×ª××™×›×” ×œ×œ×§×•×—×•×ª. ×”× ×¨×•×¦×™× ×©×”×¡×•×›×Ÿ "×™×–×›×•×¨" ×©×™×—×•×ª ×§×•×“××•×ª.

\textbf{××¨×›×™×˜×§×˜×•×¨×”:}
\begin{itemize}
  \item \textbf{PostgreSQL}: ×˜×‘×œ×” \code{customers} -- ×©×, ××™×™×œ, ×”×¢×“×¤×•×ª
  \item \textbf{ChromaDB}: Embeddings ×©×œ ×›×œ ×©×™×—×” ×¢× \code{customer\_id}
  \item \textbf{Summarization}: ×‘×¡×•×£ ×›×œ ×©×™×—×”, ×¡×›× ××•×ª×” ×•×©××•×¨ ×‘-SQL
\end{itemize}

\textbf{×ª×”×œ×™×š:}
\begin{enumerate}
  \item ×œ×§×•×— ××ª×—×‘×¨ ×¢× \code{customer\_id=123}
  \item ×©××™×œ×ª×” ×œ-PostgreSQL: ×©×œ×•×£ ×”×¢×“×¤×•×ª ×‘×¡×™×¡×™×•×ª
  \item ×©××™×œ×ª×” ×œ-ChromaDB: ××¦× 3 ×©×™×—×•×ª ×¨×œ×•×•× ×˜×™×•×ª ××”×¢×‘×¨ (×“××™×•×Ÿ ×¡×× ×˜×™ ×œ× ×•×©× ×”× ×•×›×—×™)
  \item ×‘× ×” Prompt: "××©×ª××© 123 ×”×•× ×¦××—×•× ×™, ×‘×¢×‘×¨ ×”×ª×œ×•× ×Ÿ ×¢×œ ××™×—×•×¨ ×‘××©×œ×•×—. ×”× ×” ×©×™×—×•×ª ×§×•×“××•×ª: ..."
  \item ×©×œ×— ×œ-LLM
\end{enumerate}

×ª×•×¦××”: ×”×œ×§×•×— ××¨×’×™×© "××•×‘×Ÿ" ×•×œ× ×¦×¨×™×š ×œ×—×–×•×¨ ×¢×œ ×¢×¦××•.
\end{examplebox}

\subsection{External Memory: ×’×™×©×” ×œ×™×“×¢ ×—×™×¦×•× ×™}

\textbf{External Memory} ×”×•× ×”×™×›×•×œ×ª ×©×œ ×”-LLM ×œ×’×©×ª ×œ××™×“×¢ ×©×œ× × ××¦× ×‘×ª×•×š ×”-Context Window~\cite{RAGSurvey2024,RAGSystemReview2025}. ×–×” × ×¢×©×” ×‘×“×¨×š ×›×œ×œ ×“×¨×š \term{RAG} ××• ×“×¨×š \term{Function Calling}.

\begin{itemize}
  \item \textbf{RAG}: ××—×–×¨ ××¡××›×™× ×¨×œ×•×•× ×˜×™×™× ×-Vector DB ×•×”×–×¨×§ ××•×ª× ×œ×¤×¨×•××¤×˜
  \item \textbf{Function Calling}: ××¤×©×¨ ×œ-LLM ×œ×§×¨×•× ×œ×¤×•× ×§×¦×™×•×ª ×—×™×¦×•× ×™×•×ª (API ×©×œ CRM, ERP, ××–×’ ××•×•×™×¨)
\end{itemize}

\section{Context Window: ××’×‘×œ×•×ª ×•××¡×˜×¨×˜×’×™×•×ª ×”×ª××•×“×“×•×ª}

\term{Context Window} ×”×•× ×”××’×‘×œ×” ×”×§×©×™×—×” ×‘×™×•×ª×¨ ×©×œ LLM~\cite{LostInTheMiddle2024,FoundInTheMiddle2024}. ×× ×”×©×™×—×” ×©×œ×š ×—×•×¨×’×ª ××× ×•, ×”××•×“×œ ×¤×©×•×˜ ×œ× ×™×›×•×œ ×œ×§×‘×œ ××ª ×”×§×œ×˜.

\subsection{××¡×˜×¨×˜×’×™×•×ª ×œ×”×ª××•×“×“×•×ª ×¢× Context Window ××•×’×‘×œ}

\subsubsection{Chunking ×•×”×–×¨×§×” ×—×•×–×¨×ª}

×× ×™×© ×œ×š ××¡××š ××¨×•×š ××“×™ (×œ××©×œ, ×¡×¤×¨ ×©×œ 200 ×¢××•×“×™×), ××ª×” ×™×›×•×œ ×œ×¤×¦×œ ××•×ª×• ×œ×—×œ×§×™×, ×œ×©×œ×•×— ×›×œ ×—×œ×§ ×‘× ×¤×¨×“, ×•×œ×¡×›× ××ª ×”×ª×•×¦××•×ª.

\textbf{×ª×”×œ×™×š:}
\begin{enumerate}
  \item ×—×œ×§ ××ª ×”××¡××š ×œ-10 ×—×œ×§×™×
  \item ×©×œ×— ×›×œ ×—×œ×§: "×¡×›× ××ª ×”×—×œ×§ ×”×–×”"
  \item ××¡×•×£ ××ª 10 ×”×¡×™×›×•××™×
  \item ×©×œ×— ×¡×™×›×•× ×¡×•×¤×™: "×¡×›× ××ª 10 ×”×¡×™×›×•××™× ×”××œ×” ×œ×¡×™×›×•× ××—×“"
\end{enumerate}

\subsubsection{Sliding Window ×¢× ×¡×™×›×•×}

×›××©×¨ ×”×©×™×—×” ××¨×•×›×” ××“×™, ××œ ×ª×©××•×¨ ××ª ×”×›×œ -- ×©××•×¨ ×¨×§ ××ª ×”-10 ×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª, ×•"×¡×›×" ××ª ×”×©××¨ ×œ×¤×¡×§×” ××—×ª.

\textbf{×“×•×’××”:}
\begin{itemize}
  \item ×”×•×“×¢×•×ª 1-50: ×¡×•×›××• ×œ-"×”××©×ª××© ×©××œ ×¢×œ ××•×¦×¨×™×, ×”×•× ××¢×•× ×™×™×Ÿ ×‘×˜×œ×¤×•× ×™×"
  \item ×”×•×“×¢×•×ª 51-60: ×©××•×¨×•×ª ×‘××œ×•××Ÿ
\end{itemize}

×›×š ××ª×” "×–×•×›×¨" ××ª ×”×¢×‘×¨, ××‘×œ ×œ× ××©×œ× ×¢×‘×•×¨ ×›×œ ×”×˜×•×§× ×™×.

\subsubsection{×©×™××•×© ×‘××•×“×œ ×¢× Context Window ×’×“×•×œ}

×”×“×¨×š ×”×¤×©×•×˜×” ×‘×™×•×ª×¨: ×¢×‘×•×¨ ×œ××•×“×œ ×¢× Context ×’×“×•×œ ×™×•×ª×¨.

\begin{itemize}
  \item ×× ××ª×” ××©×ª××© ×‘-GPT-3.5 (16K), ×¢×‘×•×¨ ×œ-GPT-4 Turbo (128K)
  \item ×× ××ª×” ×¦×¨×™×š ×™×•×ª×¨, ×¢×‘×•×¨ ×œ-Claude 3.5 Sonnet (200K) ××• Gemini 1.5 Pro (2M)
\end{itemize}

×–×” ×™×§×¨ ×™×•×ª×¨, ××‘×œ ×œ×¤×¢××™× ×–×” ×”×›×¨×—×™.

\section{Vendor Lock-in: ×”×¡×™×›×•×Ÿ ×”× ×¡×ª×¨}

\term{Vendor Lock-in} ×”×•× ×”××¦×‘ ×©×‘×• ×”××¨×’×•×Ÿ ×”×•×¤×š ×ª×œ×•×™ ×œ×—×œ×•×˜×™×Ÿ ×‘×¡×¤×§ ××—×“, ×•××¢×‘×¨ ×œ×¡×¤×§ ××—×¨ ×›×¨×•×š ×‘×¢×œ×•×™×•×ª ××“×™×¨×•×ª ××• ×‘×œ×ª×™ ××¤×©×¨×™ ×œ×—×œ×•×˜×™×Ÿ~\cite{AIVendorLockIn2024,MultiCloudStrategy2025}.

\subsection{××™×š × ×•×¦×¨ Vendor Lock-in ×‘××¢×¨×›×•×ª AI?}

\begin{enumerate}
  \item \textbf{Proprietary APIs}: ×©×™××•×© ×‘-API ×™×™×¢×•×“×™ ×©×œ ×¡×¤×§ ××¡×•×™× (×œ××©×œ, \code{gpt-4-vision}) ×©××™×Ÿ ×œ×• ×—×œ×•×¤×” ×‘×¡×¤×§×™× ××—×¨×™×
  \item \textbf{Fine-Tuned Models}: ××™××•×Ÿ ××•×“×œ ×™×™×¢×•×“×™ ×‘-OpenAI -- ×œ× × ×™×ª×Ÿ ×œ×”×¢×‘×™×¨ ××•×ª×• ×œ-Anthropic
  \item \textbf{Data Format Lock}: ×©××™×¨×ª × ×ª×•× ×™× ×‘×¤×•×¨××˜ ×™×™×¢×•×“×™ ×©×§×©×” ×œ×”×¢×‘×™×¨ (×œ××©×œ, Embeddings ×©×œ OpenAI ×œ× ×ª×•×××™× ×œ-Embeddings ×©×œ Cohere)
  \item \textbf{Workflow Dependency}: ×©×™××•×© ×‘×›×œ×™× ×™×™×¢×•×“ ×©×œ ×¡×¤×§ (×œ××©×œ, LangChain ×¢× OpenAI ×‘×œ×‘×“)
\end{enumerate}

\subsection{××¡×˜×¨×˜×’×™×•×ª ×œ×× ×™×¢×ª Vendor Lock-in}

\subsubsection{×©×›×‘×ª ×”×¤×©×˜×” (Abstraction Layer)}

×‘××§×•× ×œ×§×¨×•× ×™×©×™×¨×•×ª ×œ-\code{openai.ChatCompletion.create()}, ×‘× ×” ×××©×§ ×›×œ×œ×™ ×©×™×›×•×œ ×œ×§×¨×•× ×œ×›×œ ×¡×¤×§~\cite{AIMiddlewareSolution2025,ModelAgnosticPlatforms2025}.

\begin{latin}
\begin{lstlisting}[style=python, caption={Python: ×©×›×‘×ª ×”×¤×©×˜×” ×œ××•×“×œ×™×}]
class LLMProvider:
    def __init__(self, provider: str):
        self.provider = provider

    def generate(self, prompt: str) -> str:
        if self.provider == "openai":
            return self._openai_generate(prompt)
        elif self.provider == "anthropic":
            return self._anthropic_generate(prompt)
        elif self.provider == "local":
            return self._local_generate(prompt)
        else:
            raise ValueError(f"Unknown provider: {self.provider}")

    def _openai_generate(self, prompt):
        import openai
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    def _anthropic_generate(self, prompt):
        import anthropic
        client = anthropic.Anthropic()
        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
        return message.content[0].text

    def _local_generate(self, prompt):
        # Ollama or local model
        import ollama
        response = ollama.chat(model='llama3.1', messages=[
            {'role': 'user', 'content': prompt}
        ])
        return response['message']['content']

# Usage
llm = LLMProvider(provider="openai")  # Easy to switch!
result = llm.generate("××”×• AI?")
\end{lstlisting}
\end{latin}

×›×¢×ª, ×× ×ª×¨×¦×” ×œ×¢×‘×•×¨ ×-OpenAI ×œ-Anthropic, ×¤×©×•×˜ ×ª×©× ×” ××ª ×”×¤×¨××˜×¨ \code{provider} -- ×œ×œ× ×¦×•×¨×š ×‘×©×™× ×•×™ ×§×•×“ × ×•×¡×£.

\subsubsection{Multi-Model Strategy}

×‘××§×•× ×œ×”×ª×—×™×™×‘ ×œ××•×“×œ ××—×“, ×”×©×ª××© ×‘\textbf{-Multi-Model Strategy}:
\begin{itemize}
  \item ××©×™××•×ª ×§×œ×•×ª: GPT-3.5 Turbo
  \item ××©×™××•×ª ××•×¨×›×‘×•×ª: Claude 3.5 Sonnet
  \item ××©×™××•×ª ×¨×’×™×©×•×ª: Llama 3.1 (Self-Hosted)
\end{itemize}

×›×š ××ª×” ×œ× ×ª×œ×•×™ ×‘×¡×¤×§ ××—×“, ×•×’× ××¤×–×¨ ×¡×™×›×•× ×™×.

\subsubsection{×ª×™×¢×•×“ ×•××‘×—× ×™ Regression}

×›×œ ×¤×¢× ×©××ª×” ××©× ×” ×¡×¤×§, ××ª×” ×¨×•×¦×” ×œ×•×•×“× ×©×”××¢×¨×›×ª ×¢×“×™×™×Ÿ ×¢×•×‘×“×ª. ×œ×›×Ÿ, ×‘× ×” \textbf{Regression Tests}:

\begin{latin}
\begin{lstlisting}[style=python, caption={Python: ××‘×—×Ÿ Regression ×œ××•×“×œ×™×}]
def test_llm_output():
    test_cases = [
        {"prompt": "What is 2+2?", "expected_substring": "4"},
        {"prompt": "Translate 'hello' to Hebrew", "expected_substring": "×©×œ×•×"},
    ]

    providers = ["openai", "anthropic", "local"]

    for provider in providers:
        llm = LLMProvider(provider=provider)
        for case in test_cases:
            result = llm.generate(case["prompt"])
            assert case["expected_substring"] in result, \
                f"Failed for {provider}: {result}"
        print(f"{provider} passed all tests!")

test_llm_output()
\end{lstlisting}
\end{latin}

\subsection{×—×™×©×•×‘ Switching Cost}

×œ×¤× ×™ ×©××—×œ×™×˜×™× ×œ×¢×‘×•×¨ ×¡×¤×§, ×—×©×•×‘ ×œ×—×©×‘ ××ª \textbf{×¢×œ×•×ª ×”××¢×‘×¨}:

\begin{formulabox}[× ×•×¡×—×ª Switching Cost]
\[
\text{Switching Cost} = C_{\text{dev}} + C_{\text{data}} + C_{\text{downtime}} + C_{\text{training}}
\]
×›××©×¨:
\begin{itemize}
  \item $C_{\text{dev}}$: ×¢×œ×•×ª ×¤×™×ª×•×— ××—×“×© (×©×¢×•×ª ××”× ×“×¡ Ã— ×©×›×¨ ×©×¢×ª×™)
  \item $C_{\text{data}}$: ×¢×œ×•×ª ×”×¢×‘×¨×ª × ×ª×•× ×™× ×•×”××¨×ª Embeddings
  \item $C_{\text{downtime}}$: ××•×‘×“×Ÿ ×”×›× ×¡×•×ª ×‘××”×œ×š ×”××¢×‘×¨
  \item $C_{\text{training}}$: ×”×“×¨×›×ª ×”×¦×•×•×ª ×¢×œ ×”×›×œ×™ ×”×—×“×©
\end{itemize}
\end{formulabox}

\begin{examplebox}[×“×•×’××”: ×—×™×©×•×‘ Switching Cost]
×—×‘×¨×” ××©×ª××©×ª ×‘-OpenAI GPT-4 ×•×¨×•×¦×” ×œ×¢×‘×•×¨ ×œ-Claude 3.5 Sonnet.

\textbf{×—×™×©×•×‘:}
\begin{itemize}
  \item \textbf{×¤×™×ª×•×— ××—×“×©}: 3 ××”× ×“×¡×™× Ã— 2 ×©×‘×•×¢×•×ª Ã— 80 ×©×¢×•×ª Ã— \$100/×©×¢×” = \$48,000
  \item \textbf{×”×¢×‘×¨×ª × ×ª×•× ×™×}: 500GB Embeddings Ã— \$0.02/GB = \$10,000 (×¦×¨×™×š ×œ×™×¦×•×¨ ××—×“×© ×¢× Embedding ××—×¨)
  \item \textbf{Downtime}: 3 ×™××™× Ã— \$5,000/×™×•× ×”×›× ×¡×•×ª = \$15,000
  \item \textbf{×”×“×¨×›×”}: 20 ×¢×•×‘×“×™× Ã— 4 ×©×¢×•×ª Ã— \$80/×©×¢×” = \$6,400
\end{itemize}

\[
\text{Switching Cost} = 48,000 + 10,000 + 15,000 + 6,400 = \$79,400
\]

\textbf{×”×—×œ×˜×”}: ×× ×”××¢×‘×¨ ×œ-Claude ×—×•×¡×š \$3,000/×—×•×“×©, × ×§×•×“×ª ×”××™×–×•×Ÿ ×”×™× 26.5 ×—×•×“×©×™×. ×”×× ×–×” ×›×“××™? ×ª×œ×•×™ ×‘××¡×˜×¨×˜×’×™×” ××¨×•×›×ª ×”×˜×•×•×—.
\end{examplebox}

\section{×“×•×’×××•×ª ××¢×©×™×•×ª}

\subsection{×“×•×’××” 1: ××¢×‘×¨ ×-GPT-3.5 ×œ-GPT-4}

\textbf{×ª×¨×—×™×©:}
×—×‘×¨×ª SaaS ×”×©×ª××©×” ×‘-GPT-3.5 Turbo ×œ×¡×™×›×•× ×¤× ×™×•×ª ×œ×§×•×—×•×ª. ×”× ×§×™×‘×œ×• ×ª×œ×•× ×•×ª ×¢×œ ×“×™×•×§ × ××•×š -- ×”××•×“×œ "×”×—××™×¥" × ×§×•×“×•×ª ×—×©×•×‘×•×ª.

\textbf{×©×™×§×•×œ×™×:}
\begin{itemize}
  \item \textbf{×‘×™×¦×•×¢×™×}: GPT-4 ××“×•×™×§ ×™×•×ª×¨, ××š ××™×˜×™ ×¤×™ 2-3
  \item \textbf{×¢×œ×•×ª}: GPT-4 ×™×§×¨ ×¤×™ 20 (!)
  \item \textbf{× ×¤×—}: 10,000 ×¤× ×™×•×ª/×—×•×“×©, ×××•×¦×¢ 500 ×˜×•×§× ×™× ×œ×¤× ×™×™×”
\end{itemize}

\textbf{×—×™×©×•×‘ ×¢×œ×•×™×•×ª:}
\begin{itemize}
  \item GPT-3.5 Turbo: $10,000 \times 0.5K \times \$0.002 = \$10$/×—×•×“×©
  \item GPT-4: $10,000 \times 0.5K \times \$0.03 = \$150$/×—×•×“×©
\end{itemize}

\textbf{×”×—×œ×˜×”:}
×”×—×‘×¨×” ×¢×‘×¨×” ×œ-\textbf{Hybrid Strategy}:
\begin{itemize}
  \item 80\% ×¤× ×™×•×ª "×¨×’×™×œ×•×ª": GPT-3.5 Turbo (\$8/×—×•×“×©)
  \item 20\% ×¤× ×™×•×ª "××•×¨×›×‘×•×ª": GPT-4 (\$30/×—×•×“×©)
\end{itemize}

×¢×œ×•×ª ×›×•×œ×œ×ª: \$38/×—×•×“×© -- ×©×™×¤×•×¨ ××™×›×•×ª ××©××¢×•×ª×™ ×‘×¢×œ×•×ª ×¡×‘×™×¨×”.

\subsection{×“×•×’××” 2: ×‘×—×™×¨×” ×‘×™×Ÿ Claude ×œ-GPT ×œ×ª××™×›×ª ×œ×§×•×—×•×ª}

\textbf{×ª×¨×—×™×©:}
×¡×˜××¨×˜××¤ ×‘×•× ×” ×¡×•×›×Ÿ ×ª××™×›×” ×œ×§×•×—×•×ª ××•×˜×•××˜×™. ×”× ×¦×¨×™×›×™× ×œ×”×—×œ×™×˜: Claude 3.5 Sonnet ××• GPT-4?

\textbf{×‘×“×™×§×ª POC:}
×”× ×”×¨×™×¦×• 100 ×©×™×—×•×ª ×××™×ª×™×•×ª ×‘××§×‘×™×œ ×¢×œ ×©× ×™ ×”××•×“×œ×™×.

\textbf{×ª×•×¦××•×ª:}
\begin{table}[H]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule
\textbf{××“×“} & \textbf{Claude 3.5} & \textbf{GPT-4} \\
\midrule
×“×™×•×§ ×ª×©×•×‘×•×ª & 87\% & 84\% \\
×–××Ÿ ×ª×’×•×‘×” ×××•×¦×¢ & 2.3s & 4.1s \\
×¢×œ×•×ª/×©×™×—×” & \$0.015 & \$0.08 \\
×©×‘×™×¢×•×ª ×¨×¦×•×Ÿ ×œ×§×•×—×•×ª & 4.2/5 & 4.0/5 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{×”×—×œ×˜×”:}
Claude 3.5 Sonnet -- ××”×™×¨ ×™×•×ª×¨, ×–×•×œ ×™×•×ª×¨, ×•×“×™×•×§ ×˜×•×‘ ×™×•×ª×¨. GPT-4 ×œ× ×”×¦×“×™×§ ××ª ×”×¢×œ×•×ª.

\subsection{×“×•×’××” 3: ×ª×›× ×•×Ÿ ××¡×˜×¨×˜×’×™×™×ª Multi-Model}

\textbf{×ª×¨×—×™×©:}
××¨×’×•×Ÿ ×’×“×•×œ ×¨×•×¦×” ×œ×‘× ×•×ª ××¢×¨×›×ª AI ×©××˜×¤×œ×ª ×‘××’×•×•×Ÿ ××©×™××•×ª.

\textbf{××¨×›×™×˜×§×˜×•×¨×”:}
\begin{itemize}
  \item \textbf{FAQ ×•×©××œ×•×ª ×¤×©×•×˜×•×ª}: GPT-3.5 Turbo (××”×™×¨ ×•×–×•×œ)
  \item \textbf{× ×™×ª×•×— ×—×•×–×™× ×•××¡××›×™×}: Claude 3.5 Sonnet (Context ×’×“×•×œ, ×“×™×•×§ ×’×‘×•×”)
  \item \textbf{× ×ª×•× ×™× ×¨×’×™×©×™×}: Llama 3.1 70B Self-Hosted (×¤×¨×˜×™×•×ª ××œ××”)
  \item \textbf{×™×¦×™×¨×ª ×§×•×“}: GPT-4 (××¦×˜×™×™×Ÿ ×‘×§×•×“)
\end{itemize}

\textbf{×ª×•×¦××”:}
\begin{itemize}
  \item ×¢×œ×•×ª ×—×•×“×©×™×ª: \$2,500 (×œ×¢×•××ª \$8,000 ×× ×”×™×• ××©×ª××©×™× ×¨×§ ×‘-GPT-4)
  \item ××™×›×•×ª: ×’×‘×•×”×” ×‘×›×œ ×ª×—×•×
  \item ×’××™×©×•×ª: ××¤×©×¨ ×œ×”×—×œ×™×£ ×¡×¤×§ ×‘×›×œ ×ª×—×•× ×‘× ×¤×¨×“
\end{itemize}

\section{×ª×¨×’×™×œ×™×}

\subsection{×ª×¨×’×™×œ ×ª×™××•×¨×˜×™ 1: ×‘× ×™×™×ª Scorecard ×œ×”×©×•×•××ª LLMs}

\textbf{××©×™××”:}
×‘× ×” \textbf{Scorecard} ×œ×”×©×•×•××ª 4 ××•×“×œ×™×: GPT-4, Claude 3.5, Gemini 1.5 Pro, Llama 3.1 70B.

\textbf{×§×¨×™×˜×¨×™×•× ×™×:}
\begin{itemize}
  \item ×‘×™×¦×•×¢×™× (MMLU)
  \item ×¢×œ×•×ª ×œ××™×œ×™×•×Ÿ ×˜×•×§×Ÿ
  \item Context Window
  \item Latency
  \item ×¨×’×™×©×•×ª × ×ª×•× ×™× (×”×× Self-Hosted?)
\end{itemize}

×ª×Ÿ ×¦×™×•×Ÿ 1-10 ×œ×›×œ ×§×¨×™×˜×¨×™×•×Ÿ, ×•×©×§×œ×œ ×œ×¤×™ ×—×©×™×‘×•×ª ×œ××¨×’×•×Ÿ ×©×œ×š.

\textbf{×¤×ª×¨×•×Ÿ ×œ×“×•×’××”:}
\begin{table}[H]
\centering
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{×§×¨×™×˜×¨×™×•×Ÿ (××©×§×œ)} & \textbf{GPT-4} & \textbf{Claude 3.5} & \textbf{Gemini} & \textbf{Llama 3.1} \\
\midrule
×‘×™×¦×•×¢×™× (30\%) & 9 & 10 & 8 & 7 \\
×¢×œ×•×ª (25\%) & 3 & 7 & 5 & 10 \\
Context (20\%) & 7 & 9 & 10 & 7 \\
Latency (15\%) & 5 & 7 & 4 & 8 \\
×¤×¨×˜×™×•×ª (10\%) & 2 & 2 & 2 & 10 \\
\midrule
\textbf{×¦×™×•×Ÿ ×›×•×œ×œ} & 6.3 & 8.0 & 6.7 & 8.0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{×”××œ×¦×”:} Claude 3.5 ××• Llama 3.1 -- ×ª×œ×•×™ ×× ××ª×” ××•×›×Ÿ ×œ× ×”×œ Self-Hosted.

\subsection{×ª×¨×’×™×œ ×ª×™××•×¨×˜×™ 2: ×—×™×©×•×‘ Switching Cost}

\textbf{×ª×¨×—×™×©:}
×—×‘×¨×” ××©×ª××©×ª ×‘-Pinecone (Vector DB) ×•×¨×•×¦×” ×œ×¢×‘×•×¨ ×œ-Qdrant (Self-Hosted).

\textbf{× ×ª×•× ×™×:}
\begin{itemize}
  \item 10 ××™×œ×™×•×Ÿ Embeddings ×‘-Pinecone
  \item ×¢×œ×•×ª Pinecone: \$500/×—×•×“×©
  \item ×¢×œ×•×ª Qdrant Self-Hosted: \$200/×—×•×“×© (×©×¨×ª)
  \item ×–××Ÿ ×”×¢×‘×¨×” ××©×•×¢×¨: 3 ×©×‘×•×¢×•×ª (2 ××”× ×“×¡×™×)
  \item ×©×›×¨ ××”× ×“×¡: \$120/×©×¢×”
\end{itemize}

\textbf{×—×©×‘:}
\begin{enumerate}
  \item Switching Cost
  \item × ×§×•×“×ª ××™×–×•×Ÿ (×›××” ×—×•×“×©×™× ×¢×“ ×©×”××¢×‘×¨ ××©×ª×œ×?)
  \item ×”×× ×›×“××™?
\end{enumerate}

\textbf{×¤×ª×¨×•×Ÿ:}
\begin{align*}
C_{\text{dev}} &= 2 \times 3 \times 40 \times 120 = \$28,800 \\
C_{\text{data}} &= 10M \times \$0.0001 = \$1,000 \\
C_{\text{downtime}} &= 2 \times \$2,000 = \$4,000 \\
C_{\text{training}} &= 5 \times 8 \times 100 = \$4,000 \\
\text{Total} &= \$37,800
\end{align*}

×—×™×¡×›×•×Ÿ ×—×•×“×©×™: $500 - 200 = \$300$

× ×§×•×“×ª ××™×–×•×Ÿ: $37,800 / 300 = 126$ ×—×•×“×©×™× (10.5 ×©× ×™×!)

\textbf{×”×—×œ×˜×”:} ×œ× ×›×“××™ -- Switching Cost ×’×‘×•×” ××“×™.

\subsection{×ª×¨×’×™×œ ×ª×™××•×¨×˜×™ 3: ×ª×›× ×•×Ÿ ××¡×˜×¨×˜×’×™×™×ª Context Management}

\textbf{×ª×¨×—×™×©:}
××ª×” ×‘×•× ×” ×¡×•×›×Ÿ ×©×™×—×” ×œ×œ×§×•×—×•×ª. ×©×™×—×” ×××•×¦×¢×ª ×”×™× 30 ×”×•×“×¢×•×ª, ×›×œ ×”×•×“×¢×” 150 ×˜×•×§× ×™×.

\textbf{×‘×¢×™×”:}
×× ×ª×©×œ×— ××ª ×›×œ ×”×”×™×¡×˜×•×¨×™×” ×‘×›×œ ×¤×¢×, ×ª×©×œ× ×”×¨×‘×”!

\textbf{××©×™××”:}
×ª×›× ×Ÿ 3 ××¡×˜×¨×˜×’×™×•×ª Context Management ×•×—×©×‘ ××ª ×”×¢×œ×•×ª ×©×œ ×›×œ ××—×ª.

\textbf{×¤×ª×¨×•×Ÿ:}
\begin{enumerate}
  \item \textbf{Full History}: ×©×œ×— ×”×›×œ ×‘×›×œ ×¤×¢×
  \begin{itemize}
    \item ×˜×•×§× ×™× ×œ×©×™×—×”: $150 \times \frac{30 \times 31}{2} = 69,750$
    \item ×¢×œ×•×ª: $69,750 \times \$0.002 / 1000 = \$0.14$ ×œ×©×™×—×”
  \end{itemize}

  \item \textbf{Sliding Window (10 ×”×•×“×¢×•×ª ××—×¨×•× ×•×ª)}
  \begin{itemize}
    \item ×˜×•×§× ×™× ×œ×©×™×—×”: ×‘×××•×¦×¢ $150 \times 10 \times 30 = 45,000$
    \item ×¢×œ×•×ª: $45,000 \times \$0.002 / 1000 = \$0.09$ ×œ×©×™×—×”
  \end{itemize}

  \item \textbf{Summarization (×¡×›× ×›×œ 10 ×”×•×“×¢×•×ª)}
  \begin{itemize}
    \item ×˜×•×§× ×™× ×œ×©×™×—×”: $500 \text{ (summary)} + 1,500 \text{ (last 10)} = 2,000$
    \item ×¢×œ×•×ª: $2,000 \times \$0.002 / 1000 = \$0.004$ ×œ×©×™×—×”
  \end{itemize}
\end{enumerate}

\textbf{×”××œ×¦×”:} Summarization -- ×—×™×¡×›×•×Ÿ ×©×œ 97\%!

\subsection{×ª×¨×’×™×œ ×ª×™××•×¨×˜×™ 4: × ×™×ª×•×— Vendor Lock-in}

\textbf{×ª×¨×—×™×©:}
×‘×“×•×§ ××ª ×”××¨×›×™×˜×§×˜×•×¨×” ×”×‘××” ×•×–×”×” × ×§×•×“×•×ª Vendor Lock-in:

\begin{itemize}
  \item Frontend: ×©×•×œ×— ×‘×§×©×•×ª ×™×©×™×¨×•×ª ×œ-OpenAI API
  \item Embeddings: ××©×ª××© ×‘-\code{text-embedding-ada-002}
  \item Database: ×©××•×¨ ×‘-Pinecone (×¢× ×Ÿ)
  \item Fine-Tuned Model: \code{ft:gpt-3.5-turbo:company:v1}
\end{itemize}

\textbf{××©×™××”:}
\begin{enumerate}
  \item ×–×”×” 4 × ×§×•×“×•×ª Lock-in
  \item ×”×¦×¢ ×¤×ª×¨×•×Ÿ ×œ×›×œ × ×§×•×“×”
\end{enumerate}

\textbf{×¤×ª×¨×•×Ÿ:}
\begin{enumerate}
  \item \textbf{Lock-in \#1}: Frontend ×§×•×¨× ×™×©×™×¨×•×ª ×œ-OpenAI
  \begin{itemize}
    \item \textbf{×¤×ª×¨×•×Ÿ}: ×”×•×¡×£ Abstraction Layer (Backend Proxy)
  \end{itemize}

  \item \textbf{Lock-in \#2}: Embeddings ×™×™×¢×•×“×™×™× ×œ-OpenAI
  \begin{itemize}
    \item \textbf{×¤×ª×¨×•×Ÿ}: ×”×©×ª××© ×‘××•×“×œ Embedding × ×™×™×˜×¨×œ×™ (BGE-M3 Self-Hosted)
  \end{itemize}

  \item \textbf{Lock-in \#3}: Pinecone ×‘×¢× ×Ÿ
  \begin{itemize}
    \item \textbf{×¤×ª×¨×•×Ÿ}: ×¢×‘×•×¨ ×œ-Qdrant ××• Weaviate (Self-Hosted)
  \end{itemize}

  \item \textbf{Lock-in \#4}: Fine-Tuned Model ×™×™×¢×•×“×™
  \begin{itemize}
    \item \textbf{×¤×ª×¨×•×Ÿ}: ×©××•×¨ ××ª Data Training -- ××¤×©×¨ ×œ×××Ÿ ××—×“×© ×¢×œ ××•×“×œ ××—×¨
  \end{itemize}
\end{enumerate}

\subsection{×ª×¨×’×™×œ ×ª×™××•×¨×˜×™ 5: ×‘× ×™×™×ª Roadmap ×˜×›× ×•×œ×•×’×™ ×œ-3 ×©× ×™×}

\textbf{×ª×¨×—×™×©:}
××ª×” CTO ×©×œ ×¡×˜××¨×˜××¤ ×‘×ª×—×•× ×”-FinTech. ×‘× ×” Roadmap ×˜×›× ×•×œ×•×’×™ AI ×œ-3 ×©× ×™×.

\textbf{×©×œ×‘ 1 (×©× ×” 1):}
\begin{itemize}
  \item POC ×¢× GPT-4 API (××”×™×¨ ×œ×™×™×©×•×)
  \item Vector DB ×‘×¢× ×Ÿ (Pinecone)
  \item 1,000 ×œ×§×•×—×•×ª
\end{itemize}

\textbf{×©×œ×‘ 2 (×©× ×” 2):}
\begin{itemize}
  \item ××¢×‘×¨ ×œ-Multi-Model (GPT-4 + Claude)
  \item ×”×•×¡×¤×ª Self-Hosted Embedding Model (BGE-M3)
  \item 10,000 ×œ×§×•×—×•×ª
\end{itemize}

\textbf{×©×œ×‘ 3 (×©× ×” 3):}
\begin{itemize}
  \item Self-Hosted LLM (Llama 4) ×œ× ×ª×•× ×™× ×¨×’×™×©×™×
  \item Self-Hosted Vector DB (Qdrant)
  \item 100,000 ×œ×§×•×—×•×ª
\end{itemize}

\textbf{×ª×•×¦××”:}
×’××™×©×•×ª ××œ××”, ×¢×œ×•×™×•×ª ××‘×•×§×¨×•×ª, ××™×Ÿ Vendor Lock-in.

\subsection{×ª×¨×’×™×œ ×§×•×“ 6: ×”×©×•×•××ª ×‘×™×¦×•×¢×™ ××•×“×œ×™× ××•×˜×•××˜×™×ª}

\textbf{××©×™××”:}
×›×ª×•×‘ ×¡×§×¨×™×¤×˜ Python ×©××©×•×•×” ××•×˜×•××˜×™×ª ××ª ×”×‘×™×¦×•×¢×™× ×©×œ 3 ××•×“×œ×™× ×¢×œ 10 ×©××œ×•×ª.

\begin{latin}
\begin{lstlisting}[style=python, caption={Python: ×”×©×•×•××ª ×‘×™×¦×•×¢×™ ××•×“×œ×™×}]
import time
from typing import List, Dict
import openai
import anthropic
import ollama

class ModelBenchmark:
    def __init__(self):
        self.results = []

    def benchmark_openai(self, prompt: str) -> Dict:
        start = time.time()
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        latency = time.time() - start
        answer = response.choices[0].message.content
        tokens = response.usage.total_tokens
        cost = tokens * 0.002 / 1000  # $0.002 per 1K tokens

        return {
            "model": "GPT-3.5 Turbo",
            "answer": answer,
            "latency": latency,
            "tokens": tokens,
            "cost": cost
        }

    def benchmark_claude(self, prompt: str) -> Dict:
        client = anthropic.Anthropic()
        start = time.time()
        message = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
        latency = time.time() - start
        answer = message.content[0].text
        tokens = message.usage.input_tokens + message.usage.output_tokens
        cost = (message.usage.input_tokens * 0.003 +
                message.usage.output_tokens * 0.015) / 1000

        return {
            "model": "Claude 3.5 Sonnet",
            "answer": answer,
            "latency": latency,
            "tokens": tokens,
            "cost": cost
        }

    def benchmark_ollama(self, prompt: str) -> Dict:
        start = time.time()
        response = ollama.chat(model='llama3.1', messages=[
            {'role': 'user', 'content': prompt}
        ])
        latency = time.time() - start
        answer = response['message']['content']

        return {
            "model": "Llama 3.1 (Local)",
            "answer": answer,
            "latency": latency,
            "tokens": 0,  # Local - no token tracking
            "cost": 0  # Self-hosted - no API cost
        }

    def run_benchmark(self, prompts: List[str]):
        for i, prompt in enumerate(prompts):
            print(f"\n=== Question {i+1}: {prompt[:50]}... ===")

            # Test all models
            for benchmark_func in [self.benchmark_openai,
                                   self.benchmark_claude,
                                   self.benchmark_ollama]:
                try:
                    result = benchmark_func(prompt)
                    self.results.append({
                        "question": i+1,
                        **result
                    })
                    print(f"{result['model']}: "
                          f"{result['latency']:.2f}s, "
                          f"${result['cost']:.4f}")
                except Exception as e:
                    print(f"Error with {benchmark_func.__name__}: {e}")

        self.print_summary()

    def print_summary(self):
        print("\n=== SUMMARY ===")
        models = set([r['model'] for r in self.results])

        for model in models:
            model_results = [r for r in self.results if r['model'] == model]
            avg_latency = sum([r['latency'] for r in model_results]) / len(model_results)
            total_cost = sum([r['cost'] for r in model_results])

            print(f"\n{model}:")
            print(f"  Avg Latency: {avg_latency:.2f}s")
            print(f"  Total Cost: ${total_cost:.4f}")

# Usage
benchmark = ModelBenchmark()

test_prompts = [
    "What is 2+2?",
    "Explain quantum computing in simple terms.",
    "Write a short poem about AI.",
    "Translate 'Hello World' to Hebrew.",
    "What are the benefits of cloud computing?",
    "Summarize the French Revolution in 2 sentences.",
    "What is the capital of Australia?",
    "Explain the difference between AI and ML.",
    "Write a Python function to calculate factorial.",
    "What are the main causes of climate change?"
]

benchmark.run_benchmark(test_prompts)
\end{lstlisting}
\end{latin}

\textbf{×ª×•×¦××” ×¦×¤×•×™×”:}
\begin{verbatim}
=== SUMMARY ===

GPT-3.5 Turbo:
  Avg Latency: 1.2s
  Total Cost: $0.0150

Claude 3.5 Sonnet:
  Avg Latency: 2.8s
  Total Cost: $0.0320

Llama 3.1 (Local):
  Avg Latency: 0.8s
  Total Cost: $0.0000
\end{verbatim}

\subsection{×ª×¨×’×™×œ ×§×•×“ 7: × ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×©×™×—×” ×¢× ×¡×™×›×•×}

\textbf{××©×™××”:}
×›×ª×•×‘ ××¢×¨×›×ª ×©×× ×”×œ×ª ×–×™×›×¨×•×Ÿ ×©×™×—×” ××¨×•×›×” ×‘×××¦×¢×•×ª ×¡×™×›×•××™× ××•×˜×•××˜×™×™×.

\begin{latin}
\begin{lstlisting}[style=python, caption={Python: × ×™×”×•×œ ×–×™×›×¨×•×Ÿ ×¢× ×¡×™×›×•×}]
import openai
from typing import List, Dict

class ConversationMemoryManager:
    def __init__(self, max_messages: int = 10):
        self.messages: List[Dict] = []
        self.summary: str = ""
        self.max_messages = max_messages

    def add_message(self, role: str, content: str):
        """Add a new message to conversation history"""
        self.messages.append({"role": role, "content": content})

        # If exceeded max messages, summarize and truncate
        if len(self.messages) > self.max_messages:
            self._summarize_and_truncate()

    def _summarize_and_truncate(self):
        """Summarize old messages and keep only recent ones"""
        print("ğŸ“ Summarizing old messages...")

        # Take first half of messages to summarize
        messages_to_summarize = self.messages[:self.max_messages // 2]

        # Create summary prompt
        conversation_text = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in messages_to_summarize
        ])

        summary_prompt = f"""Summarize this conversation in 2-3 sentences:

{conversation_text}

Previous summary: {self.summary if self.summary else 'None'}

Provide a concise summary that captures key points."""

        # Generate summary
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": summary_prompt}],
            max_tokens=200
        )

        new_summary = response.choices[0].message.content

        # Update summary
        if self.summary:
            self.summary = f"{self.summary}\n\n{new_summary}"
        else:
            self.summary = new_summary

        # Keep only recent messages
        self.messages = self.messages[self.max_messages // 2:]

        print(f"âœ… Summary updated. Keeping {len(self.messages)} recent messages.")

    def get_context_for_llm(self) -> List[Dict]:
        """Get full context to send to LLM"""
        context = []

        # Add summary as system message if exists
        if self.summary:
            context.append({
                "role": "system",
                "content": f"Previous conversation summary:\n{self.summary}"
            })

        # Add recent messages
        context.extend(self.messages)

        return context

    def chat(self, user_message: str) -> str:
        """Send message and get response"""
        # Add user message
        self.add_message("user", user_message)

        # Get context
        context = self.get_context_for_llm()

        # Call LLM
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=context
        )

        assistant_message = response.choices[0].message.content

        # Add assistant message
        self.add_message("assistant", assistant_message)

        return assistant_message

    def get_stats(self) -> Dict:
        """Get memory statistics"""
        total_tokens = sum([
            len(msg['content'].split()) * 1.3  # rough estimate
            for msg in self.messages
        ])

        return {
            "total_messages": len(self.messages),
            "summary_length": len(self.summary.split()) if self.summary else 0,
            "estimated_tokens": int(total_tokens)
        }

# Usage Example
memory = ConversationMemoryManager(max_messages=10)

# Simulate long conversation
conversation = [
    "What is machine learning?",
    "Can you give me an example?",
    "How does it differ from deep learning?",
    "What are neural networks?",
    "Explain backpropagation.",
    "What is gradient descent?",
    "How do you prevent overfitting?",
    "What is cross-validation?",
    "Explain the bias-variance tradeoff.",
    "What are ensemble methods?",
    "Tell me about random forests.",
    "How does XGBoost work?",
    "What is feature engineering?",
    "Explain dimensionality reduction.",
    "What is PCA?"
]

for user_msg in conversation:
    print(f"\nğŸ‘¤ User: {user_msg}")
    response = memory.chat(user_msg)
    print(f"ğŸ¤– Assistant: {response[:100]}...")

    # Print stats every 5 messages
    if len(memory.messages) % 5 == 0:
        stats = memory.get_stats()
        print(f"\nğŸ“Š Stats: {stats}")

# Final summary
print("\n" + "="*50)
print("FINAL SUMMARY:")
print(memory.summary)
print("\nğŸ“Š Final Stats:", memory.get_stats())
\end{lstlisting}
\end{latin}

\textbf{×”×¡×‘×¨ ×”×§×•×“:}
\begin{enumerate}
  \item ×”××¢×¨×›×ª ×©×•××¨×ª ×¨×§ ××ª ×”-10 ×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª
  \item ×›×©×¢×•×‘×¨×™× ××ª ×”××’×‘×œ×”, ×”×™× ××¡×›××ª ××ª ×”××—×¦×™×ª ×”×¨××©×•× ×”
  \item ×”×¡×™×›×•× × ×©×œ×— ×›-System Message ×‘×›×œ ×§×¨×™××”
  \item ×›×š ×—×•×¡×›×™× ×˜×•×§× ×™× ××‘×œ ×©×•××¨×™× ×¢×œ ×”×§×©×¨
\end{enumerate}

\section{×¡×™×›×•×}

×‘×¤×¨×§ ×–×” ×œ××“× ×• ×›×™×¦×“ ×œ×§×‘×œ ×”×—×œ×˜×•×ª ××¡×˜×¨×˜×’×™×•×ª ×‘×¢×•×œ× ×”-AI ×”×¢×¡×§×™~\cite{GartnerAIGateway2024}. ×¨××™× ×• ×©×‘×—×™×¨×ª \term{LLM} ××™× ×” ×¨×§ ×©××œ×” ×˜×›× ×™×ª -- ×”×™× ×”×—×œ×˜×” ×¢×¡×§×™×ª ×©××©×¤×™×¢×” ×¢×œ ×¢×œ×•×™×•×ª, ×‘×™×¦×•×¢×™×, ×¤×¨×˜×™×•×ª ×•×’××™×©×•×ª ×¢×ª×™×“×™×ª.

\textbf{× ×§×•×“×•×ª ××¤×ª×—:}
\begin{itemize}
  \item \textbf{××™×Ÿ ××•×“×œ "××•×©×œ×"} -- ×›×œ ××•×“×œ ××¦×˜×™×™×Ÿ ×‘××©×™××•×ª ××¡×•×™××•×ª. Multi-Model Strategy ×”×™× ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ×”×¤×ª×¨×•×Ÿ ×”×˜×•×‘ ×‘×™×•×ª×¨.
  \item \textbf{Context Window ×”×•× ××’×‘×œ×” ×§×¨×™×˜×™×ª} -- ×ª×›× ×Ÿ ××¨××© ××™×š ××ª×” ×× ×”×œ ×–×™×›×¨×•×Ÿ, ×•×‘×—×¨ ××•×“×œ ×¢× Context ××ª××™× ×œ××©×™××•×ª ×©×œ×š.
  \item \textbf{Vendor Lock-in ×”×•× ×¡×™×›×•×Ÿ ×××™×ª×™} -- ×”×©×§×™×¢×• ×‘×©×›×‘×•×ª ×”×¤×©×˜×” ×•×‘××¨×›×™×˜×§×˜×•×¨×” ×’××™×©×” ××”×™×•× ×”×¨××©×•×Ÿ.
  \item \textbf{×—×©×‘×• ××¨×•×š ×˜×•×•×—} -- Switching Cost ×™×›×•×œ ×œ×”×™×•×ª ×¢×¦×•×. ×ª×›× × ×• Roadmap ×˜×›× ×•×œ×•×’×™ ×œ-3 ×©× ×™× ×œ×¤×—×•×ª.
\end{itemize}

×‘×¤×¨×§ ×”×‘× × ×¢×¡×•×§ ×‘×××©×§×™ ××©×ª××© ×•××™× ×˜×¨××§×¦×™×” -- ××™×š ×œ×”×¤×•×š ××ª ×›×œ ×”×˜×›× ×•×œ×•×’×™×” ×”×–×• ×œ×—×•×•×™×™×ª ××©×ª××© ××¢×•×œ×”.
